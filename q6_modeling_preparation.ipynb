{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170920c8",
   "metadata": {},
   "source": [
    "# Q6: Modeling Preparation\n",
    "\n",
    "**Phase 7:** Modeling Preparation  \n",
    "**Points: 3 points**\n",
    "\n",
    "**Focus:** Perform temporal train/test split, select features, handle categorical variables.\n",
    "\n",
    "**Lecture Reference:** Lecture 11, Notebook 3 ([`11/demo/03_pattern_analysis_modeling_prep.ipynb`](https://github.com/christopherseaman/datasci_217/blob/main/11/demo/03_pattern_analysis_modeling_prep.ipynb)), Phase 7. This notebook demonstrates temporal train/test splitting (see \"Your Approach\" section below for the key code pattern).\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3645955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 196,479 records with features\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load feature-engineered data from Q4\n",
    "df = pd.read_csv('output/q4_features.csv', parse_dates=['Measurement Timestamp'], index_col='Measurement Timestamp')\n",
    "# Or if you saved without index:\n",
    "# df = pd.read_csv('output/q4_features.csv')\n",
    "# df['Measurement Timestamp'] = pd.to_datetime(df['Measurement Timestamp'])\n",
    "# df = df.set_index('Measurement Timestamp')\n",
    "print(f\"Loaded {len(df):,} records with features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc8e59",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Prepare data for modeling by performing temporal train/test split, selecting features, and handling categorical variables.\n",
    "\n",
    "**CRITICAL - Temporal Split:** For time series data, you **MUST** use temporal splitting (earlier data for training, later data for testing). **DO NOT** use random split. Why? Time series data has temporal dependencies - using future data to predict the past would be data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "## Required Artifacts\n",
    "\n",
    "You must create exactly these 5 files in the `output/` directory:\n",
    "\n",
    "### 1. `output/q6_X_train.csv`\n",
    "**Format:** CSV file\n",
    "**Content:** Training features (X)\n",
    "**Requirements:**\n",
    "- All feature columns (no target variable)\n",
    "- Only training data (earlier time periods)\n",
    "- **No index column** (save with `index=False`)\n",
    "- **No datetime column** (unless it's a feature, not the index)\n",
    "\n",
    "### 2. `output/q6_X_test.csv`\n",
    "**Format:** CSV file\n",
    "**Content:** Test features (X)\n",
    "**Requirements:**\n",
    "- All feature columns (same as X_train)\n",
    "- Only test data (later time periods)\n",
    "- **No index column** (save with `index=False`)\n",
    "- **No datetime column** (unless it's a feature, not the index)\n",
    "\n",
    "### 3. `output/q6_y_train.csv`\n",
    "**Format:** CSV file\n",
    "**Content:** Training target variable (y)\n",
    "**Requirements:**\n",
    "- Single column with target variable name as header\n",
    "- Only training data (corresponding to X_train)\n",
    "- **No index column** (save with `index=False`)\n",
    "\n",
    "**Example:**\n",
    "```csv\n",
    "Water Temperature\n",
    "15.2\n",
    "15.3\n",
    "15.1\n",
    "...\n",
    "```\n",
    "\n",
    "### 4. `output/q6_y_test.csv`\n",
    "**Format:** CSV file\n",
    "**Content:** Test target variable (y)\n",
    "**Requirements:**\n",
    "- Single column with target variable name as header\n",
    "- Only test data (corresponding to X_test)\n",
    "- **No index column** (save with `index=False`)\n",
    "\n",
    "### 5. `output/q6_train_test_info.txt`\n",
    "**Format:** Plain text file\n",
    "**Content:** Train/test split information\n",
    "**Required information:**\n",
    "- Split method: Temporal (80/20 or similar)\n",
    "- Training set size: [number] samples\n",
    "- Test set size: [number] samples\n",
    "- Training date range: [start] to [end]\n",
    "- Test date range: [start] to [end]\n",
    "- Number of features: [number]\n",
    "- Target variable: [name]\n",
    "\n",
    "**Example format:**\n",
    "```\n",
    "TRAIN/TEST SPLIT INFORMATION\n",
    "==========================\n",
    "\n",
    "Split Method: Temporal (80/20 split by time)\n",
    "\n",
    "Training Set Size: 40000 samples\n",
    "Test Set Size: 10000 samples\n",
    "\n",
    "Training Date Range: 2022-01-01 00:00:00 to 2026-09-15 07:00:00\n",
    "Test Date Range: 2026-09-15 08:00:00 to 2027-09-15 07:00:00\n",
    "\n",
    "Number of Features: 22\n",
    "Target Variable: Water Temperature\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements Checklist\n",
    "\n",
    "- [ ] Target variable selected\n",
    "- [ ] Temporal train/test split performed (train on earlier data, test on later data - **NOT random split**)\n",
    "- [ ] Features selected and prepared\n",
    "- [ ] Categorical variables handled (encoding if needed)\n",
    "- [ ] No data leakage (future data not in training set)\n",
    "- [ ] All 5 required artifacts saved with exact filenames\n",
    "\n",
    "---\n",
    "\n",
    "## Your Approach\n",
    "\n",
    "1. **Select target variable** - Choose a meaningful numeric variable to predict\n",
    "2. **Select features** - Exclude target, non-numeric columns, and any features derived from the target (to avoid data leakage)\n",
    "3. **Handle categorical variables** - One-hot encode if needed\n",
    "4. **Perform temporal train/test split** - Sort by datetime, then split by index position (earlier data for training, later for testing)\n",
    "5. **Save artifacts** - Save X_train, X_test, y_train, y_test as separate CSVs\n",
    "6. **Document split** - Record split sizes, date ranges, and feature count\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Selection Guidelines\n",
    "\n",
    "When selecting features for modeling, think critically about each feature:\n",
    "\n",
    "**Red Flags to Watch For:**\n",
    "- **Circular logic**: Does this feature use the target variable to predict the target?\n",
    "  - Example: Rolling mean of target, lag of target (if not handled carefully)\n",
    "  - Example: If predicting `Air Temperature`, using `air_temp_rolling_7h` is circular - you're predicting temperature from smoothed temperature\n",
    "- **Data leakage**: Does this feature contain information that wouldn't be available at prediction time?\n",
    "  - Example: Future values, aggregated statistics that include the current value\n",
    "- **Near-duplicates**: Is this feature nearly identical to the target?\n",
    "  - Check correlations - if correlation > 0.95, investigate whether it's legitimate\n",
    "  - Example: A feature with 99%+ correlation with the target is likely problematic\n",
    "\n",
    "**Good Practices:**\n",
    "- Use external predictors (other weather variables, temporal features)\n",
    "- Create rolling windows of **predictors**, not the target\n",
    "  - Good: `wind_speed_rolling_7h`, `humidity_rolling_24h`\n",
    "  - Bad: `air_temp_rolling_7h` when predicting Air Temperature\n",
    "- Use derived features that combine multiple predictors\n",
    "- Think: \"Would I have this information when making a real prediction?\"\n",
    "\n",
    "**Remember:** The goal is to predict the target from **other** information, not from the target itself.\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Points\n",
    "\n",
    "- **Target variable:** What do you want to predict? Temperature? Water conditions? Choose something meaningful and measurable.\n",
    "- **Temporal split:** **CRITICAL** - Use temporal split (earlier data for training, later data for testing), NOT random split. Why? Time series data has temporal dependencies. Typical split: 80/20 or 70/30.\n",
    "- **Feature selection:** Which features are most relevant? Consider correlations, domain knowledge, and feature importance from previous analysis.\n",
    "- **Categorical encoding:** If you have categorical variables, encode them (one-hot encoding, label encoding, etc.) before modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "After Q6, you should have:\n",
    "- [ ] Temporal train/test split completed (earlier → train, later → test)\n",
    "- [ ] Features prepared (no target, no datetime index)\n",
    "- [ ] Categorical variables encoded\n",
    "- [ ] No data leakage verified\n",
    "- [ ] All 5 artifacts saved: `q6_X_train.csv`, `q6_X_test.csv`, `q6_y_train.csv`, `q6_y_test.csv`, `q6_train_test_info.txt`\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Continue to `q7_modeling.md` for Modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ea3414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Target Variable\n",
    "# =========================\n",
    "target_var = \"Air Temperature\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9417411a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features and target separated\n",
      "Number of features: 21\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Feature Selection\n",
    "# =========================\n",
    "\n",
    "# Drop target from feature set\n",
    "X = df.drop(columns=[target_var])\n",
    "\n",
    "# Remove any accidental datetime columns\n",
    "X = X.select_dtypes(exclude=[\"datetime64[ns]\"])\n",
    "\n",
    "# Separate target\n",
    "y = df[target_var]\n",
    "\n",
    "print(\"✅ Features and target separated\")\n",
    "print(\"Number of features:\", X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40c95aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Station Name', 'Measurement Timestamp Label', 'Measurement ID'], dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "categorical_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e895ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Safe categorical encoding complete\n",
      "New feature count: 22\n"
     ]
    }
   ],
   "source": [
    "# ✅ SAFE categorical columns only (low cardinality)\n",
    "safe_categorical_cols = [\"Station Name\"]  # add others ONLY if small categories\n",
    "\n",
    "# ✅ One-hot encode ONLY these columns\n",
    "X = pd.get_dummies(X, columns=safe_categorical_cols, drop_first=True)\n",
    "\n",
    "print(\"✅ Safe categorical encoding complete\")\n",
    "print(\"New feature count:\", X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a82c3c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ High-cardinality ID columns removed\n"
     ]
    }
   ],
   "source": [
    "# ❌ Drop any unique identifiers that should never be modeled\n",
    "bad_id_cols = [\"Measurement ID\", \"Measurement Timestamp Label\"]\n",
    "\n",
    "X = X.drop(columns=[c for c in bad_id_cols if c in X.columns])\n",
    "\n",
    "print(\"✅ High-cardinality ID columns removed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3140836c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Wet Bulb Temperature',\n",
       " 'Humidity',\n",
       " 'Rain Intensity',\n",
       " 'Interval Rain',\n",
       " 'Total Rain',\n",
       " 'Precipitation Type',\n",
       " 'Wind Direction',\n",
       " 'Wind Speed',\n",
       " 'Maximum Wind Speed',\n",
       " 'Barometric Pressure',\n",
       " 'Solar Radiation',\n",
       " 'Heading',\n",
       " 'Battery Life',\n",
       " 'wind_speed_squared',\n",
       " 'is_raining',\n",
       " 'pressure_change',\n",
       " 'solar_radiation_normalized',\n",
       " 'wind_humidity_interaction',\n",
       " 'Station Name_Foster Weather Station',\n",
       " 'Station Name_Oak Street Weather Station']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View all final feature names\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "print(f\"Total number of features: {len(feature_names)}\")\n",
    "feature_names[:30]  # preview first 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e35575c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dropped features based on leakage & redundancy control:\n",
      " - Wet Bulb Temperature\n",
      " - Heading\n",
      " - Battery Life\n",
      " - wind_speed_squared\n",
      " - Solar Radiationpressure_change\n",
      "✅ Final feature count: 16\n"
     ]
    }
   ],
   "source": [
    "features_to_drop = [\n",
    "    \"Wet Bulb Temperature\",   # true leakage\n",
    "    \"Heading\",                # physically meaningless\n",
    "    \"Battery Life\",           # sensor health variable\n",
    "    \"wind_speed_squared\",    # redundant derived feature\n",
    "    \"Solar Radiation\"        # keeping normalized version instead\n",
    "    \"pressure_change\"\n",
    "]\n",
    "\n",
    "X = X.drop(columns=features_to_drop, errors=\"ignore\")\n",
    "\n",
    "print(\"✅ Dropped features based on leakage & redundancy control:\")\n",
    "for f in features_to_drop:\n",
    "    print(\" -\", f)\n",
    "\n",
    "print(\"✅ Final feature count:\", X.shape[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "450a30ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Temporal train/test split completed\n",
      "Train size: 137535\n",
      "Test size: 58944\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Temporal Train/Test Split (70/30)\n",
    "# =========================\n",
    "\n",
    "split_index = int(len(df) * 0.7)\n",
    "\n",
    "X_train = X.iloc[:split_index]\n",
    "X_test = X.iloc[split_index:]\n",
    "\n",
    "y_train = y.iloc[:split_index]\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "print(\"✅ Temporal train/test split completed\")\n",
    "print(\"Train size:\", X_train.shape[0])\n",
    "print(\"Test size:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6af6833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All Q6 train/test CSV files saved\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Save Train/Test CSV Files\n",
    "# =========================\n",
    "\n",
    "X_train.to_csv(\"output/q6_X_train.csv\", index=False)\n",
    "X_test.to_csv(\"output/q6_X_test.csv\", index=False)\n",
    "\n",
    "y_train.to_csv(\"output/q6_y_train.csv\", index=False)\n",
    "y_test.to_csv(\"output/q6_y_test.csv\", index=False)\n",
    "\n",
    "print(\"✅ All Q6 train/test CSV files saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3e8e369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ output/q6_train_test_info.txt saved\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Save Train/Test Info Text File\n",
    "# =========================\n",
    "\n",
    "train_start = df.index.min()\n",
    "train_end = df.index[split_index - 1]\n",
    "test_start = df.index[split_index]\n",
    "test_end = df.index.max()\n",
    "\n",
    "train_test_info = [\n",
    "    \"TRAIN/TEST SPLIT INFORMATION\",\n",
    "    \"==========================\",\n",
    "    \"\",\n",
    "    \"Split Method: Temporal (70/30 split by time)\",\n",
    "    \"\",\n",
    "    f\"Training Set Size: {len(X_train)} samples\",\n",
    "    f\"Test Set Size: {len(X_test)} samples\",\n",
    "    \"\",\n",
    "    f\"Training Date Range: {train_start} to {train_end}\",\n",
    "    f\"Test Date Range: {test_start} to {test_end}\",\n",
    "    \"\",\n",
    "    f\"Number of Features: {X_train.shape[1]}\",\n",
    "    f\"Target Variable: {target_var}\"\n",
    "]\n",
    "\n",
    "with open(\"output/q6_train_test_info.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(train_test_info))\n",
    "\n",
    "print(\"✅ output/q6_train_test_info.txt saved\")\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
